'''
Logistic regression and feature selection

In this exercise we'll perform feature selection on the movie review sentiment data set using L1 regularization. The features and targets are already loaded for you in X_train and y_train. We'll search for the best value of C using scikit-learn's GridSearchCV, which was covered in the prerequisite course.
'''

import scipy.sparse as sps
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from scipy.sparse import hstack, vstack

X_train = vstack((sps.load_npz('../datasets/X_train.csr.npz'),
                  sps.load_npz('../datasets/X_test.csr.npz')))

y_train = hstack((sps.load_npz('../datasets/y_train.csr.npz'),
                  sps.load_npz('../datasets/y_test.csr.npz'))).data

'''
INSTRUCTIONS

*   Instantiate a logistic regression object that uses L1 regularization.
*   Find the value of C that minimizes cross-validation error.
*   Print out the number of selected features for this value of C.
'''

# Specify L1 regularization
lr = LogisticRegression(penalty='l1')

# Instantiate the GridSearchCV object and run the search
searcher = GridSearchCV(lr, {'C':[0.001, 0.01, 0.1, 1, 10]})
searcher.fit(X_train, y_train)

# Report the best parameters
print("Best CV params", searcher.best_params_)

# Find the number of nonzero coefficients (selected features)
best_lr = searcher.best_estimator_
coefs = best_lr.coef_
print("Total number of features:", coefs.size)
print("Number of selected features:", np.count_nonzero(coefs))